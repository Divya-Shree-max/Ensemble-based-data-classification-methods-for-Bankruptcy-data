# -*- coding: utf-8 -*-
"""bankruptcy_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gpUXMl7sqWEsIJu8qMBRr3vh3gpE2ZsZ
"""

# Bankruptcy Prediction using Ensemble Methods on Imbalanced Data

# ğŸ“¦ Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
import xgboost as xgb

# ğŸ“‚ Load Dataset
df = pd.read_csv('data/bankruptcy_data.csv')  # Adjust path if needed

# ğŸ§¹ Data Check
print("Missing Values:\n", df.isnull().sum())
print("\nTarget Value Counts:\n", df['Bankrupt?'].value_counts())

# ğŸ“Š Class Distribution
sns.countplot(data=df, x='Bankrupt?')
plt.title("Class Imbalance (0 = Not Bankrupt, 1 = Bankrupt)")
plt.show()

# ğŸ§¾ Split Features and Labels
X = df.drop('Bankrupt?', axis=1)
y = df['Bankrupt?']

# ğŸ”€ Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# ğŸ“ Scale Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ğŸ” Handle Imbalance with SMOTE + Undersampling
over = SMOTE(sampling_strategy=0.3, random_state=42)
under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)
pipeline = Pipeline(steps=[('o', over), ('u', under)])
X_resampled, y_resampled = pipeline.fit_resample(X_train_scaled, y_train)

# ğŸŒ² Random Forest Model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_resampled, y_resampled)
rf_preds = rf.predict(X_test_scaled)

# ğŸŒ¿ Gradient Boosting Model
gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_resampled, y_resampled)
gb_preds = gb.predict(X_test_scaled)

# âš¡ XGBoost Model
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_resampled, y_resampled)
xgb_preds = xgb_model.predict(X_test_scaled)

# ğŸ§ª Evaluation Function
def evaluate_model(name, y_true, y_pred):
    print(f"\nğŸ“Œ {name} Results:")
    print("Classification Report:")
    print(classification_report(y_true, y_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))
    print("ROC-AUC Score:", roc_auc_score(y_true, y_pred))

# ğŸ“ˆ Evaluate All Models
evaluate_model("Random Forest", y_test, rf_preds)
evaluate_model("Gradient Boosting", y_test, gb_preds)
evaluate_model("XGBoost", y_test, xgb_preds)